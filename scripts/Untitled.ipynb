{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from models.scnn_pytorch.model import SCNN\n",
    "from models.erfnet.erfnet import ERFNet\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_window_region = np.array([600.0 / 1920, 280.0 / 1080, 1000.0 / 1920, 360.0 / 1080])\n",
    "\n",
    "\n",
    "def GetRectFromRegion(region: np.array, width, height):\n",
    "    x = float(width) * region[0]\n",
    "    y = float(height) * region[1]\n",
    "    region_width = float(width) * region[2]\n",
    "    region_height = float(height) * region[3]\n",
    "    return np.array([x, y, region_width, region_height], dtype=np.int)\n",
    "\n",
    "rect = GetRectFromRegion(drive_window_region, 1920, 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = '../models/scnn_pytorch'\n",
    "INPUT_SIZE = (800, 288)\n",
    "net = SCNN(input_size=INPUT_SIZE, pretrained=False, test=True)\n",
    "state = torch.load(MODEL_DIR + '/exp10_best.pth')\n",
    "net.load_state_dict(state['net'])\n",
    "net.eval().cuda().half()\n",
    "\n",
    "mean = (0.3598, 0.3653, 0.3662)\n",
    "std = (0.2573, 0.2663, 0.2756)\n",
    "\n",
    "cv2.namedWindow('result')\n",
    "\n",
    "for filename in os.listdir('../images'):\n",
    "    img = cv2.imread('../images/' + filename)\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    drive_window_img = img[rect[1]:rect[1] + rect[3], rect[0]:rect[0] + rect[2]]\n",
    "    input_blob = drive_window_img.astype(np.float)\n",
    "    input_blob = input_blob / 255.0\n",
    "    input_blob = (input_blob - mean) / std\n",
    "    input_blob = cv2.resize(input_blob, INPUT_SIZE)\n",
    "    input_blob = input_blob.transpose([2, 0, 1])\n",
    "\n",
    "    input_tensor = torch.from_numpy(np.array([input_blob]))\n",
    "    input_tensor = input_tensor.float().cuda().half()\n",
    "\n",
    "    seg, exist = net(input_tensor)\n",
    "    seg_img = seg.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    seg_img = cv2.cvtColor(seg_img, cv2.COLOR_GRAY2RGB)\n",
    "    seg_img = cv2.resize(seg_img, (1280, 720), interpolation=cv2.INTER_LINEAR)\n",
    "    drive_window_img = cv2.resize(drive_window_img, (1280, 720))\n",
    "    blend = cv2.addWeighted(seg_img * 50, 0.5, drive_window_img, 0.8, 0)\n",
    "\n",
    "    cv2.imshow('result', blend)\n",
    "    if cv2.waitKey(500) == 27:\n",
    "        break\n",
    "cv2.destroyWindow('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ERFNet(5)\n",
    "input_mean = net.input_mean\n",
    "input_std = net.input_std\n",
    "net= torch.nn.DataParallel(net, device_ids=[0]).cuda()\n",
    "checkpoint = torch.load('D:/Dev/Codes-for-Lane-Detection/ERFNet-CULane-PyTorch/trained/ERFNet_trained.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('result')\n",
    "cv2.moveWindow('result', 200,200)\n",
    "for filename in os.listdir('../images'):\n",
    "    img = cv2.imread('../images/' + filename)\n",
    "\n",
    "    drive_window_img = img[rect[1]:rect[1] + rect[3], rect[0]:rect[0] + rect[2]-200]\n",
    "    input_blob = drive_window_img.astype(np.float)\n",
    "    input_blob = input_blob\n",
    "    input_blob = (input_blob - input_mean) / input_std\n",
    "    input_blob = cv2.resize(input_blob, (976, 208))\n",
    "    input_blob = input_blob.transpose([2, 0, 1])\n",
    "\n",
    "    input_tensor = torch.from_numpy(np.array([input_blob]))\n",
    "    input_tensor = input_tensor.float().cuda()\n",
    "\n",
    "    seg, exist = net(input_tensor)\n",
    "\n",
    "    seg = seg.detach().cpu().numpy()\n",
    "    seg_img = np.argmax(seg[0], axis=0).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    seg_img = cv2.cvtColor(seg_img, cv2.COLOR_GRAY2RGB)\n",
    "    seg_img = cv2.resize(seg_img, (1280, 720), interpolation=cv2.INTER_LINEAR)\n",
    "    drive_window_img = cv2.resize(drive_window_img, (1280, 720))\n",
    "    blend = cv2.addWeighted(seg_img * 50, 0.5, drive_window_img, 0.8, 0)\n",
    "\n",
    "    cv2.imshow('result', blend)\n",
    "    if cv2.waitKey(500) == 27:\n",
    "        break\n",
    "cv2.destroyWindow('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[103.939, 116.779, 123.68]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../models/PINet')\n",
    "from models.PINet import agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = agent.Agent()\n",
    "state = torch.load('../models/PINet/savefile/640_tensor(0.2298)_lane_detection_network.pkl')\n",
    "net.lane_detection_network.load_state_dict(state)\n",
    "net.cuda()\n",
    "net.evaluate_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread(\"../images/1591496736089.png\")\n",
    "test_image = cv2.resize(test_image, (512,256))/255.0\n",
    "input_blob = test_image.transpose([2, 0, 1])\n",
    "input_blob = np.array([input_blob])\n",
    "\n",
    "confidences, offsets, instances = net.predict_lanes_test(input_blob)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2dc68933f40>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confidences = (confidences[0][0].detach().cpu().numpy())*50\n",
    "confidences = confidences.astype(np.uint8)\n",
    "plt.imshow(confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('result')\n",
    "cv2.moveWindow('result', 200,200)\n",
    "for filename in os.listdir('../images'):\n",
    "    img = cv2.imread('../images/' + filename)\n",
    "\n",
    "    drive_window_img = img[rect[1]:rect[1] + rect[3], rect[0]:rect[0] + rect[2]-200]\n",
    "    input_blob = drive_window_img.astype(np.float)\n",
    "    input_blob = cv2.resize(input_blob, (512,256)) / 255.0\n",
    "    input_blob = input_blob.transpose([2, 0, 1])\n",
    "\n",
    "    input_tensor = np.array([input_blob])\n",
    "\n",
    "    confidences, offsets, instances = net.predict_lanes_test(input_tensor)[-1]\n",
    "\n",
    "    confidences = (confidences[0][0].detach().cpu().numpy())*50\n",
    "    confidences = confidences.astype(np.uint8)\n",
    "#     seg_img = np.argmax(seg[0], axis=0).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    confidences = cv2.cvtColor(confidences, cv2.COLOR_GRAY2RGB)\n",
    "    seg_img = cv2.resize(confidences, (1280, 720), interpolation=cv2.INTER_NEAREST)\n",
    "    drive_window_img = cv2.resize(drive_window_img, (1280, 720))\n",
    "    blend = cv2.addWeighted(seg_img * 50, 0.5, drive_window_img, 0.8, 0)\n",
    "\n",
    "    cv2.imshow('result', blend)\n",
    "    if cv2.waitKey(500) == 27:\n",
    "        break\n",
    "cv2.destroyWindow('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
